# yo-xing.github.io
## Final Project for MUSIGR6610 - SOUND: ADVANCED TOPICS at Columbia University
### Yo Jeremijenko-Conley  

#### Motivation
In recent months, the role of Artificial Intelligence (AI) in art has become increasingly prevalent. Software like DALL-E 2, MidJourney, and DiffusionBee have enabled users to generate images based on textual descriptions. This software uses generative AI to create brand-new images (or artworks) that had previously never existed, based on datasets of millions of existing images/artwork. These software services have slightly different implementations, but their overall structure is relatively similar. For example, The DALL-E 2 model works as follows: 

First, the text prompt given by the user is fed into a trained text encoder model which maps the prompt into a representation space. After that, an OpenAI model called CLIP (Contrastive Language-Image Pre-Training) maps the representation space of the text encoding to a corresponding image representation space, CLIP is trained on hundreds of millions of images with associated captions, and learns how related any given piece of text is to an image. This captures the semantic information of the text prompt. A model called the prior then takes the CLIP embedding and generates a specific image embedding from it, you can think of it as forming a “mental image” of the text. Lastly, a diffusion model is used to stochastically generate the image by removing noise (or upsampling) from the image embedding. Diffusion models are trained with existing images with gaussian noise added to them iteratively by a Markov chain, which eventually leaves the image as nothing but noise. The diffusion model then learns how to iteratively reverse this noise to generate a (low-resolution) image from seemingly random noise. However, this general diffusion process does not correspond to a specific text prompt. To fix this, DALL-E 2 uses a modified version of their diffusion model (called GLIDE) which incorporates projected CLIP text embeddings into this process. At each time step in the iterative process of noise removal, the CLIP text embeddings are added to the image embedding at that specific time-step to “guide” the removal of noise in the direction of the texts representation space, classifier free guidance is also used to further guide the image towards the text prompt. 

![1*_wtvEU05qyDISFBT4xRVgw](https://user-images.githubusercontent.com/40434203/208201013-bfaeac92-8ea9-4773-82b0-4c582fcfee48.png)

  
  The results of these machine learning processes have been incredibly impressive in their ability to capture and translate semantic information and their grasp of aesthetics and art styles. This has sparked a discourse on whether or not these models are exploitative of artists whose work is being used to train these models, and among other things, their potential to generate artwork in other mediums, eventually becoming a cheap alternative to replace creative professionals. After experiencing the capabilities of these models, I viewed them as a “proof of concept” to creative applications of AI in the arts. This made me consider the possibilities of this technology being used to generate music, as well as the cultural and economic implications of this. In this project, I attempt to examine the current state of music-generating AI as well as speculate on its future capabilities and impact on the world. 
Some may assume that AI models would be better able to generate music and capture its stylistic patterns than those in paintings and visual arts due to the inherent mathematical structure of the basis of most music (such as the diatonic scale). However, the current capabilities of AI models to generate music are far behind their capability to generate visual art in the form of images. This is mainly due to two reasons, the first being the sequential nature of music, the interpretation of each tone or sound in a song is reliant on those that came before it and necessary for the interpretation of later sounds in the piece. While this can also be said for images, the non-sequential nature of how we experience them allows us to discount specific sections of an image easier than parts of a song. The second reason for the disparity between AI-generated images and music is the immense size of an audio file compared to an image, this makes encoding and decoding audio vastly more computationally expensive than doing so for an image. 
